{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997c7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f62476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b46705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e5a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c356cd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pouri\\anaconda3\\envs\\langchain_env\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'all_docs_zotero/chroma/'\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbb2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Publication Year\",\n",
    "        description=\"The year that the paper was published.\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Date Added\",\n",
    "        description=\"The year that the paper was added to the collection.\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Author\",\n",
    "        description=\"Authors of the paper, it could be couple of people.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Title\", \n",
    "        description=\"Title of the paper that the paper is about.\", \n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Brain Heart Interconnectome (BHI) research papers\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    #search_kwargs={\"k\": 10}\n",
    "    #enable_limit=True,\n",
    ")\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912669c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, compression_retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44fdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know.\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38602f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dadd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 30650b85-4450-4ca1-9f54-aff8120c88ca not found for run f44674b4-73e7-496c-8ed6-7467cd93acf0. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The CONSORT Statement is a key tool for achieving adequate reporting in research. It includes a checklist and flow diagram that journals can incorporate into their review processes to ensure adherence. Endorsement of CONSORT by journals is crucial for its effectiveness, and it is defined as editorial statements endorsing CONSORT, requirements in journal instructions to authors, or requirements for authors to submit CONSORT checklists and flow diagrams. Studies have shown that journals endorsing CONSORT have higher completeness of reports compared to non-endorsing journals.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What can you tell me about consort?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },\n",
    ")['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb74e7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 254eb357-0daa-40fe-81d7-d06786ea438c not found for run f2587c02-9513-419e-8ca7-72d1766c6f1b. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The CONSORT Statement, which stands for Consolidated Standards of Reporting Trials, was developed to improve the reporting of randomized controlled trials (RCTs). It consists of a 25-item checklist and a flow diagram that provide guidance on how to report various aspects of an RCT, such as the study design, participant recruitment, interventions, outcomes, and statistical analysis. Adherence to the CONSORT guidelines helps ensure transparency and completeness in reporting, which is essential for evaluating the validity and reliability of trial results. Journals are encouraged to endorse the CONSORT guidelines and require authors to follow them when submitting RCT manuscripts for publication.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Tell me more\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },\n",
    ")['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb0923c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         +-----------------------------+                      \r\n",
      "                         | Parallel<chat_history>Input |                      \r\n",
      "                         +-----------------------------+                      \r\n",
      "                                ***            ***                            \r\n",
      "                              **                  **                          \r\n",
      "                            **                      **                        \r\n",
      "            +------------------------+          +-------------+               \r\n",
      "            | Lambda(_enter_history) |          | Passthrough |               \r\n",
      "            +------------------------+          +-------------+               \r\n",
      "                                ***            ***                            \r\n",
      "                                   **        **                               \r\n",
      "                                     **    **                                 \r\n",
      "                        +------------------------------+                      \r\n",
      "                        | Parallel<chat_history>Output |                      \r\n",
      "                        +------------------------------+                      \r\n",
      "                                        *                                     \r\n",
      "                                        *                                     \r\n",
      "                                        *                                     \r\n",
      "                           +------------------------+                         \r\n",
      "                           | Parallel<context>Input |                         \r\n",
      "                           +------------------------+                         \r\n",
      "                                  **         ***                              \r\n",
      "                                **              *                             \r\n",
      "                               *                 **                           \r\n",
      "                        +--------+          +-------------+                   \r\n",
      "                        | Branch |          | Passthrough |                   \r\n",
      "                        +--------+          +-------------+                   \r\n",
      "                                  **         ***                              \r\n",
      "                                    **      *                                 \r\n",
      "                                      *   **                                  \r\n",
      "                           +-------------------------+                        \r\n",
      "                           | Parallel<context>Output |                        \r\n",
      "                           +-------------------------+                        \r\n",
      "                                        *                                     \r\n",
      "                                        *                                     \r\n",
      "                                        *                                     \r\n",
      "                            +-----------------------+                         \r\n",
      "                            | Parallel<answer>Input |                         \r\n",
      "                            +-----------------------+                         \r\n",
      "                              ***                   *****                     \r\n",
      "                           ***                           *****                \r\n",
      "                         **                                   ****            \r\n",
      "         +------------------------+                               ***         \r\n",
      "         | Parallel<context>Input |                                 *         \r\n",
      "         +------------------------+                                 *         \r\n",
      "               **           **                                      *         \r\n",
      "             **               **                                    *         \r\n",
      "           **                   **                                  *         \r\n",
      "+----------------+          +-------------+                         *         \r\n",
      "| PromptTemplate |          | Passthrough |                         *         \r\n",
      "+----------------+          +-------------+                         *         \r\n",
      "               **           **                                      *         \r\n",
      "                 **       **                                        *         \r\n",
      "                   **   **                                          *         \r\n",
      "         +-------------------------+                                *         \r\n",
      "         | Parallel<context>Output |                                *         \r\n",
      "         +-------------------------+                                *         \r\n",
      "                      *                                             *         \r\n",
      "                      *                                             *         \r\n",
      "                      *                                             *         \r\n",
      "           +--------------------+                                   *         \r\n",
      "           | ChatPromptTemplate |                                   *         \r\n",
      "           +--------------------+                                   *         \r\n",
      "                      *                                             *         \r\n",
      "                      *                                             *         \r\n",
      "                      *                                             *         \r\n",
      "               +------------+                                       *         \r\n",
      "               | ChatOpenAI |                                       *         \r\n",
      "               +------------+                                       *         \r\n",
      "                      *                                             *         \r\n",
      "                      *                                             *         \r\n",
      "                      *                                             *         \r\n",
      "             +-----------------+                             +-------------+  \r\n",
      "             | StrOutputParser |                             | Passthrough |  \r\n",
      "             +-----------------+                         ****+-------------+  \r\n",
      "                              ***                    ****                     \r\n",
      "                                 ***            *****                         \r\n",
      "                                    **       ***                              \r\n",
      "                           +------------------------+                         \r\n",
      "                           | Parallel<answer>Output |                         \r\n",
      "                           +------------------------+                         \n"
     ]
    }
   ],
   "source": [
    "conversational_rag_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c755ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptTemplate(input_variables=['page_content'], template='{page_content}'),\n",
       " ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\n\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c41df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36b5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
